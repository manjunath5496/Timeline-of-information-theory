
<h2 id="Timeline of information theory">Timeline of information theory </h2>

<ul>
<li>1872&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann" target="_blank" rel="nofollow noopener">Ludwig Boltzmann</a>&nbsp;presents his&nbsp;<a href="https://en.wikipedia.org/wiki/H-theorem" target="_blank" rel="nofollow noopener">H-theorem</a>, and with it the formula&nbsp;for the entropy of a single gas particle</li>
<li>1878&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/J._Willard_Gibbs" target="_blank" rel="nofollow noopener">J. Willard Gibbs</a>&nbsp;defines the&nbsp;<a href="https://en.wikipedia.org/wiki/Gibbs_entropy" target="_blank" rel="nofollow noopener">Gibbs entropy</a>: the probabilities in the entropy formula are now taken as probabilities of the state of the&nbsp;whole&nbsp;system</li>
<li>1924&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Harry_Nyquist" target="_blank" rel="nofollow noopener">Harry Nyquist</a>&nbsp;discusses quantifying "<strong>intelligence</strong>" and the speed at which it can be transmitted by a communication system</li>
<li>1927&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/John_von_Neumann" target="_blank" rel="nofollow noopener">John von Neumann</a>&nbsp;defines the&nbsp;<a href="https://en.wikipedia.org/wiki/Von_Neumann_entropy" target="_blank" rel="nofollow noopener">von Neumann entropy</a>, extending the Gibbs entropy to quantum mechanics</li>
<li>1928&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Ralph_Hartley" target="_blank" rel="nofollow noopener">Ralph Hartley</a>&nbsp;introduces&nbsp;<a href="https://en.wikipedia.org/wiki/Hartley_information" target="_blank" rel="nofollow noopener">Hartley information</a>&nbsp;as the logarithm of the number of possible messages, with information being communicated when the receiver can distinguish one sequence of symbols from any other (regardless of any associated meaning)</li>
<li>1929&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Le%C3%B3_Szil%C3%A1rd" target="_blank" rel="nofollow noopener">Le&oacute; Szil&aacute;rd</a>&nbsp;analyses&nbsp;<a href="https://en.wikipedia.org/wiki/Maxwell%27s_Demon" target="_blank" rel="nofollow noopener">Maxwell's Demon</a>, showing how a&nbsp;<a href="https://en.wikipedia.org/wiki/Szilard_engine" target="_blank" rel="nofollow noopener">Szilard engine</a>&nbsp;can sometimes transform information into the extraction of useful work</li>
<li>1940&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Alan_Turing" target="_blank" rel="nofollow noopener">Alan Turing</a>&nbsp;introduces the&nbsp;<a href="https://en.wikipedia.org/wiki/Deciban" target="_blank" rel="nofollow noopener">deciban</a>&nbsp;as a measure of information inferred about the German&nbsp;<a href="https://en.wikipedia.org/wiki/Enigma_machine" target="_blank" rel="nofollow noopener">Enigma machine</a>&nbsp;cypher settings by the&nbsp;<a href="https://en.wikipedia.org/wiki/Banburismus" target="_blank" rel="nofollow noopener">Banburismus</a>&nbsp;process</li>
<li>1944&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Claude_Shannon" target="_blank" rel="nofollow noopener">Claude Shannon</a>'s theory of information is substantially complete</li>
<li>1947&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Richard_W._Hamming" target="_blank" rel="nofollow noopener">Richard W. Hamming</a>&nbsp;invents&nbsp;<a href="https://en.wikipedia.org/wiki/Hamming_code" target="_blank" rel="nofollow noopener">Hamming codes</a>&nbsp;for error detection and correction (to protect patent rights, the result is not published until 1950)</li>
<li>1948&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Claude_E._Shannon" target="_blank" rel="nofollow noopener">Claude E. Shannon</a>&nbsp;publishes&nbsp;<a href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" target="_blank" rel="nofollow noopener">A Mathematical Theory of Communication</a></li>
<li>1949&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Claude_E._Shannon" target="_blank" rel="nofollow noopener">Claude E. Shannon</a>&nbsp;publishes&nbsp;Communication in the Presence of Noise&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" target="_blank" rel="nofollow noopener">Nyquist&ndash;Shannon sampling theorem</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_law" target="_blank" rel="nofollow noopener">Shannon&ndash;Hartley law</a></li>
<li>1949&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Claude_E._Shannon" target="_blank" rel="nofollow noopener">Claude E. Shannon</a>'s&nbsp;<a href="https://en.wikipedia.org/wiki/Communication_Theory_of_Secrecy_Systems" target="_blank" rel="nofollow noopener">Communication Theory of Secrecy Systems</a>&nbsp;is declassified</li>
<li>1949&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Robert_M._Fano" target="_blank" rel="nofollow noopener">Robert M. Fano</a>&nbsp;publishes&nbsp;Transmission of Information. M.I.T. Press, Cambridge, Massachusetts &ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding" target="_blank" rel="nofollow noopener">Shannon&ndash;Fano coding</a></li>
<li>1949&nbsp;&nbsp;&ndash; Leon G. Kraft discovers&nbsp;<a href="https://en.wikipedia.org/wiki/Kraft%27s_inequality" target="_blank" rel="nofollow noopener">Kraft's inequality</a>, which shows the limits of&nbsp;<a href="https://en.wikipedia.org/wiki/Prefix_codes" target="_blank" rel="nofollow noopener">prefix codes</a></li>
<li>1949&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Marcel_J._E._Golay" target="_blank" rel="nofollow noopener">Marcel J. E. Golay</a>&nbsp;introduces&nbsp;<a href="https://en.wikipedia.org/wiki/Golay_code_(disambiguation)" target="_blank" rel="nofollow noopener">Golay codes</a>&nbsp;for&nbsp;<a href="https://en.wikipedia.org/wiki/Forward_error_correction" target="_blank" rel="nofollow noopener">forward error correction</a></li>
<li>1951&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Solomon_Kullback" target="_blank" rel="nofollow noopener">Solomon Kullback</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Richard_Leibler" target="_blank" rel="nofollow noopener">Richard Leibler</a>&nbsp;introduce the&nbsp;<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="nofollow noopener">Kullback&ndash;Leibler divergence</a></li>
<li>1951&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/David_A._Huffman" target="_blank" rel="nofollow noopener">David A. Huffman</a>&nbsp;invents&nbsp;<a href="https://en.wikipedia.org/wiki/Huffman_encoding" target="_blank" rel="nofollow noopener">Huffman encoding</a>, a method of finding optimal&nbsp;<a href="https://en.wikipedia.org/wiki/Prefix_codes" target="_blank" rel="nofollow noopener">prefix codes</a>&nbsp;for&nbsp;<a href="https://en.wikipedia.org/wiki/Lossless" target="_blank" rel="nofollow noopener">lossless</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Data_compression" target="_blank" rel="nofollow noopener">data compression</a></li>
<li>1953&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/w/index.php?title=August_Albert_Sardinas&amp;action=edit&amp;redlink=1" target="_blank" rel="nofollow noopener">August Albert Sardinas</a>&nbsp;and George W. Patterson devise the&nbsp;<a href="https://en.wikipedia.org/wiki/Sardinas%E2%80%93Patterson_algorithm" target="_blank" rel="nofollow noopener">Sardinas&ndash;Patterson algorithm</a>, a procedure to decide whether a given&nbsp;<a href="https://en.wikipedia.org/wiki/Variable-length_code" target="_blank" rel="nofollow noopener">variable-length code</a>&nbsp;is uniquely decodable</li>
<li>1954&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Irving_S._Reed" target="_blank" rel="nofollow noopener">Irving S. Reed</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/David_E._Muller" target="_blank" rel="nofollow noopener">David E. Muller</a>&nbsp;propose&nbsp;<a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Muller_code" target="_blank" rel="nofollow noopener">Reed&ndash;Muller codes</a></li>
<li>1955&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Peter_Elias" target="_blank" rel="nofollow noopener">Peter Elias</a>&nbsp;introduces&nbsp;<a href="https://en.wikipedia.org/wiki/Convolutional_code" target="_blank" rel="nofollow noopener">convolutional codes</a></li>
<li>1957&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Eugene_Prange" target="_blank" rel="nofollow noopener">Eugene Prange</a>&nbsp;first discusses&nbsp;<a href="https://en.wikipedia.org/wiki/Cyclic_code" target="_blank" rel="nofollow noopener">cyclic codes</a></li>
<li>1959&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Alexis_Hocquenghem" target="_blank" rel="nofollow noopener">Alexis Hocquenghem</a>, and independently the next year&nbsp;<a href="https://en.wikipedia.org/wiki/Raj_Chandra_Bose" target="_blank" rel="nofollow noopener">Raj Chandra Bose</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Dwijendra_Kumar_Ray-Chaudhuri" target="_blank" rel="nofollow noopener">Dwijendra Kumar Ray-Chaudhuri</a>, discover&nbsp;<a href="https://en.wikipedia.org/wiki/BCH_code" target="_blank" rel="nofollow noopener">BCH codes</a></li>
<li>1960&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Irving_S._Reed" target="_blank" rel="nofollow noopener">Irving S. Reed</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Gustave_Solomon" target="_blank" rel="nofollow noopener">Gustave Solomon</a>&nbsp;propose&nbsp;<a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_code" target="_blank" rel="nofollow noopener">Reed&ndash;Solomon codes</a></li>
<li>1962&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Robert_G._Gallager" target="_blank" rel="nofollow noopener">Robert G. Gallager</a>&nbsp;proposes&nbsp;<a href="https://en.wikipedia.org/wiki/Low-density_parity-check_code" target="_blank" rel="nofollow noopener">low-density parity-check codes</a>; they are unused for 30 years due to technical limitations</li>
<li>1965&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Dave_Forney" target="_blank" rel="nofollow noopener">Dave Forney</a>&nbsp;discusses&nbsp;<a href="https://en.wikipedia.org/wiki/Concatenated_code" target="_blank" rel="nofollow noopener">concatenated codes</a></li>
<li>1966&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Fumitada_Itakura" target="_blank" rel="nofollow noopener">Fumitada Itakura</a>&nbsp;(<a href="https://en.wikipedia.org/wiki/Nagoya_University" target="_blank" rel="nofollow noopener">Nagoya University</a>) and Shuzo Saito (<a href="https://en.wikipedia.org/wiki/Nippon_Telegraph_and_Telephone" target="_blank" rel="nofollow noopener">Nippon Telegraph and Telephone</a>) develop&nbsp;<a href="https://en.wikipedia.org/wiki/Linear_predictive_coding" target="_blank" rel="nofollow noopener">linear predictive coding</a>&nbsp;(LPC), a form of&nbsp;<a href="https://en.wikipedia.org/wiki/Speech_coding" target="_blank" rel="nofollow noopener">speech coding</a></li>
<li>1967&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Andrew_Viterbi" target="_blank" rel="nofollow noopener">Andrew Viterbi</a>&nbsp;reveals the&nbsp;<a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="nofollow noopener">Viterbi algorithm</a>, making decoding of convolutional codes practicable</li>
<li>1968&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Elwyn_Berlekamp" target="_blank" rel="nofollow noopener">Elwyn Berlekamp</a>&nbsp;invents the&nbsp;<a href="https://en.wikipedia.org/wiki/Berlekamp%E2%80%93Massey_algorithm" target="_blank" rel="nofollow noopener">Berlekamp&ndash;Massey algorithm</a>; its application to decoding BCH and Reed&ndash;Solomon codes is pointed out by&nbsp;<a href="https://en.wikipedia.org/wiki/James_Lee_Massey" target="_blank" rel="nofollow noopener">James L. Massey</a>&nbsp;the following year</li>
<li>1968&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Chris_Wallace_(computer_scientist)" target="_blank" rel="nofollow noopener">Chris Wallace</a>&nbsp;and David M. Boulton publish the first of many papers on&nbsp;<a href="https://en.wikipedia.org/wiki/Minimum_Message_Length" target="_blank" rel="nofollow noopener">Minimum Message Length</a>&nbsp;(<a href="https://en.wikipedia.org/wiki/Minimum_Message_Length" target="_blank" rel="nofollow noopener">MML</a>) statistical and inductive inference</li>
<li>1970&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Valerii_Denisovich_Goppa" target="_blank" rel="nofollow noopener">Valerii Denisovich Goppa</a>&nbsp;introduces&nbsp;<a href="https://en.wikipedia.org/wiki/Goppa_code" target="_blank" rel="nofollow noopener">Goppa codes</a></li>
<li>1972&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/w/index.php?title=J%C3%B8rn_Justesen&amp;action=edit&amp;redlink=1" target="_blank" rel="nofollow noopener">J&oslash;rn Justesen</a>&nbsp;proposes&nbsp;<a href="https://en.wikipedia.org/wiki/Justesen_code" target="_blank" rel="nofollow noopener">Justesen codes</a>, an improvement of Reed&ndash;Solomon codes</li>
<li>1972&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/N._Ahmed" target="_blank" rel="nofollow noopener">Nasir Ahmed</a>&nbsp;proposes the&nbsp;<a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform" target="_blank" rel="nofollow noopener">discrete cosine transform</a>&nbsp;(DCT), which he develops with T. Natarajan and&nbsp;<a href="https://en.wikipedia.org/wiki/K._R._Rao" target="_blank" rel="nofollow noopener">K. R. Rao</a>&nbsp;in 1973;&nbsp;the DCT later became the most widely used&nbsp;<a href="https://en.wikipedia.org/wiki/Lossy_compression" target="_blank" rel="nofollow noopener">lossy compression</a>&nbsp;algorithm, the basis for multimedia formats such as&nbsp;<a href="https://en.wikipedia.org/wiki/JPEG" target="_blank" rel="nofollow noopener">JPEG</a>,&nbsp;<a href="https://en.wikipedia.org/wiki/MPEG" target="_blank" rel="nofollow noopener">MPEG</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/MP3" target="_blank" rel="nofollow noopener">MP3</a></li>
<li>1973&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/David_Slepian" target="_blank" rel="nofollow noopener">David Slepian</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Jack_Keil_Wolf" target="_blank" rel="nofollow noopener">Jack Wolf</a>&nbsp;discover and prove the&nbsp;<a href="https://en.wikipedia.org/wiki/Slepian%E2%80%93Wolf_coding" target="_blank" rel="nofollow noopener">Slepian&ndash;Wolf coding</a>&nbsp;limits for distributed&nbsp;<a href="https://en.wikipedia.org/wiki/Source_coding" target="_blank" rel="nofollow noopener">source coding</a></li>
<li>1976&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Gottfried_Ungerboeck" target="_blank" rel="nofollow noopener">Gottfried Ungerboeck</a>&nbsp;gives the first paper on&nbsp;<a href="https://en.wikipedia.org/wiki/Trellis_modulation" target="_blank" rel="nofollow noopener">trellis modulation</a>; a more detailed exposition in 1982 leads to a raising of analogue modem&nbsp;<a href="https://en.wikipedia.org/wiki/Plain_old_telephone_service" target="_blank" rel="nofollow noopener">POTS</a>&nbsp;speeds from 9.6 kbit/s to 33.6 kbit/s</li>
<li>1976&nbsp;&nbsp;&ndash; Richard Pasco and&nbsp;<a href="https://en.wikipedia.org/wiki/Jorma_J._Rissanen" target="_blank" rel="nofollow noopener">Jorma J. Rissanen</a>&nbsp;develop effective&nbsp;<a href="https://en.wikipedia.org/wiki/Arithmetic_coding" target="_blank" rel="nofollow noopener">arithmetic coding</a>&nbsp;techniques</li>
<li>1977&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Abraham_Lempel" target="_blank" rel="nofollow noopener">Abraham Lempel</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Jacob_Ziv" target="_blank" rel="nofollow noopener">Jacob Ziv</a>&nbsp;develop Lempel&ndash;Ziv compression (<a href="https://en.wikipedia.org/wiki/LZ77" target="_blank" rel="nofollow noopener">LZ77</a>)</li>
<li>1989&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Phil_Katz" target="_blank" rel="nofollow noopener">Phil Katz</a>&nbsp;publishes&nbsp;<a href="https://en.wikipedia.org/wiki/ZIP_(file_format)" target="_blank" rel="nofollow noopener">the&nbsp;.zip&nbsp;format</a>&nbsp;including&nbsp;<a href="https://en.wikipedia.org/wiki/DEFLATE" target="_blank" rel="nofollow noopener">DEFLATE</a>&nbsp;(LZ77 + Huffman coding); later to become the most widely used archive container</li>
<li>1993&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Claude_Berrou" target="_blank" rel="nofollow noopener">Claude Berrou</a>,&nbsp;<a href="https://en.wikipedia.org/wiki/Alain_Glavieux" target="_blank" rel="nofollow noopener">Alain Glavieux</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Punya_Thitimajshima" target="_blank" rel="nofollow noopener">Punya Thitimajshima</a>&nbsp;introduce&nbsp;<a href="https://en.wikipedia.org/wiki/Turbo_code" target="_blank" rel="nofollow noopener">Turbo codes</a></li>
<li>1994&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Michael_Burrows" target="_blank" rel="nofollow noopener">Michael Burrows</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/David_Wheeler_(computer_scientist)" target="_blank" rel="nofollow noopener">David Wheeler</a>&nbsp;publish the&nbsp;<a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform" target="_blank" rel="nofollow noopener">Burrows&ndash;Wheeler transform</a>, later to find use in&nbsp;<a href="https://en.wikipedia.org/wiki/Bzip2" target="_blank" rel="nofollow noopener">bzip2</a></li>
<li>1995&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Benjamin_Schumacher" target="_blank" rel="nofollow noopener">Benjamin Schumacher</a>&nbsp;coins the term&nbsp;<a href="https://en.wikipedia.org/wiki/Qubit" target="_blank" rel="nofollow noopener">qubit</a>&nbsp;and proves the quantum noiseless coding theorem</li>
<li>2006&nbsp;&nbsp;&ndash; first&nbsp;<a href="https://en.wikipedia.org/wiki/Asymmetric_numeral_systems" target="_blank" rel="nofollow noopener">Asymmetric numeral systems</a>&nbsp;entropy coding: since 2014 popular replacement of&nbsp;<a href="https://en.wikipedia.org/wiki/Huffman_coding" target="_blank" rel="nofollow noopener">Huffman</a>&nbsp;and&nbsp;<a href="https://en.wikipedia.org/wiki/Arithmetic_coding" target="_blank" rel="nofollow noopener">arithmetic coding</a>&nbsp;in compressors like&nbsp;<a href="https://en.wikipedia.org/wiki/Zstandard" target="_blank" rel="nofollow noopener">Facebook Zstandard</a>&nbsp;or&nbsp;<a href="https://en.wikipedia.org/wiki/LZFSE" target="_blank" rel="nofollow noopener">Apple LZFSE</a></li>
<li>2008&nbsp;&nbsp;&ndash;&nbsp;<a href="https://en.wikipedia.org/wiki/Erdal_Ar%C4%B1kan" target="_blank" rel="nofollow noopener">Erdal ArÄ±kan</a>&nbsp;introduces&nbsp;<a href="https://en.wikipedia.org/wiki/Polar_code_(coding_theory)" target="_blank" rel="nofollow noopener">polar codes</a>, the first practical construction of codes that achieves capacity for a wide array of channels</li>
</ul>
</br>
